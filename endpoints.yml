# This file contains the different endpoints your bot can use.

# Server where the models are pulled from.
# https://rasa.com/docs/rasa/model-storage#fetching-models-from-a-server

#models:
#  url: http://my-server.com/models/default_core@latest
#  wait_time_between_pulls:  10   # [optional](default: 100)

# Server which runs your custom actions.
# https://rasa.com/docs/rasa/custom-actions

action_endpoint:
  url: "http://localhost:5055/webhook"

# Tracker store which is used to store the conversations.
# By default the conversations are stored in memory.
# https://rasa.com/docs/rasa/tracker-stores

#tracker_store:
#    type: redis
#    url: <host of the redis instance, e.g. localhost>
#    port: <port of your redis instance, usually 6379>
#    db: <number of your database within redis, e.g. 0>
#    password: <password used for authentication>
#    use_ssl: <whether or not the communication is encrypted, default false>

#tracker_store:
#    type: mongod
#    url: <url to your mongo instance, e.g. mongodb://localhost:27017>
#    db: <name of the db within your mongo instance, e.g. rasa>
#    username: <username used for authentication>
#    password: <password used for authentication>

# Event broker which all conversation events should be streamed to.
# https://rasa.com/docs/rasa/event-brokers

#event_broker:
#  url: localhost
#  username: username
#  password: password
#  queue: queue

nlg:
  type: rephrase
  rephrase_all: false

vector_store:
  url: "https://api.kapa.ai/query/v1"
  kapa_endpoint_type: "search"
  # kapa_project_id: ${KAPA_PROJECT_ID}
  # kapa_token: ${KAPA_TOKEN}
  kapa_num_results: 8

model_groups:
  - id: gpt4o
    models:
      - provider: openai
        model: gpt-4o
        cache:
          no-cache: true

  - id: sonnet-37
    models:
      - provider: bedrock
        model: anthropic.claude-3-7-sonnet-20250219-v1:0
        model_id: arn:aws:bedrock:us-east-2:961893475800:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0
        aws_region_name: us-east-2
        timeout: 8

  - id: sonnet-35
    models:
      - provider: bedrock
        model: anthropic.claude-3-5-sonnet-20241022-v2:0
        model_id: arn:aws:bedrock:ap-southeast-2:505147427416:application-inference-profile/drlqrsfnwy1r
        aws_region_name: ap-southeast-2
        timeout: 8

  - id: sonnet-35-20241022-v2
    models:
      - provider: bedrock
        model: anthropic.claude-3-5-sonnet-20241022-v2:0
        model_id: arn:aws:bedrock:us-east-2:961893475800:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0
        aws_region_name: us-east-2
        timeout: 8
  - id: nova-pro
    models:
      - provider: bedrock
        model: amazon.nova-pro-v1:0
        model_id: arn:aws:bedrock:us-east-2:961893475800:inference-profile/us.amazon.nova-pro-v1:0
        aws_region_name: us-east-2
        timeout: 8
  - id: sonnet-35-20241022-v2-ap
    models:
      - provider: bedrock
        model: anthropic.claude-3-5-sonnet-20241022-v2:0
        aws_region_name: ap-southeast-2
        timeout: 8

  - id: sonnet-4
    models:
      - provider: bedrock
        model: anthropic.claude-sonnet-4-20250514-v1:0
        aws_region_name: ap-southeast-2
        timeout: 8

  - id: mistral-large-2402-ap
    models:
      - provider: bedrock
        model: mistral.mistral-large-2402-v1:0
        aws_region_name: ap-southeast-2
        timeout: 8
  - id: gpt4-direct
    models:
      - provider: openai
        model: gpt-4
  - id: gpt4o-direct
    models:
      - provider: openai
        model: gpt-4o

  - id: phi4
    models:
      - provider: deepinfra
        model: microsoft/phi-4

  - id: deepseek-v3
    models:
      - provider: deepinfra
        model: deepseek-ai/DeepSeek-V3

  - id: deepseek-r1
    models:
      - provider: deepinfra
        model: deepseek-ai/DeepSeek-R1

  - id: DeepSeek-R1-Distill-Llama-70B
    models:
      - provider: deepinfra
        model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
        temperature: 0.0

  - id: Llama-4-Maverick-17B-128E-Instruct-FP8
    # $0.20/$0.60 in/out Mtoken
    models:
      - provider: deepinfra
        model: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8

  - id: Llama-3.3-70B-Instruct
    # $0.23/$0.40 in/out Mtoken
    models:
      - provider: deepinfra
        model: meta-llama/Llama-3.3-70B-Instruct

  - id: Meta-Llama-3.1-405B-Instruct
    # $0.80 / Mtoken
    models:
      - provider: deepinfra
        model: meta-llama/Meta-Llama-3.1-405B-Instruct

  - id: Qwen2.5-72B-Instruct
    # $0.13/$0.40 in/out Mtoken
    models:
      - provider: deepinfra
        model: Qwen/Qwen2.5-72B-Instruct

  - id: Qwen2.5-Coder-32B-Instruct
    models:
      - provider: deepinfra
        model: Qwen/Qwen2.5-Coder-32B-Instruct

  - id: Qwen2.5-7B-Instruct
    models:
      - provider: deepinfra
        model: Qwen/Qwen2.5-7B-Instruct

  - id: Qwen2.5-Omni-7B
    models:
      - provider: deepinfra
        model: rgstephens/qwen-omni

  - id: DeepSeek-R1-Distill-Qwen-32B
    models:
      - provider: deepinfra
        model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B

  - id: gemma-3-27b-it
    models:
      - provider: deepinfra
        model: google/gemma-3-27b-it

  - id: gemma-3-4b-it
    models:
      - provider: self-hosted
        api_base: http://localhost:8000/v1 # instead of "model_path: "/path/to/model.bin""
        model: google/gemma-3-4b-it
        # model: gemma-3-4b-it-GGUF/gemma-3-4b-it-Q4_K_M.gguf

  - id: deepinfra-gemma-3-27b-it
    models:
      - provider: deepinfra
        model: google/gemma-3-27b-it

  - id: Mistral-Small-24B-Instruct-2501
    models:
      - provider: deepinfra
        model: mistralai/Mistral-Small-24B-Instruct-2501

   # GEMINI_API_KEY
  - id: gemini-2.5-pro-preview-05-06
    models:
      - provider: gemini
        model: gemini-2.5-pro-preview-05-06

  - id: gemini-2.5-flash-preview-04-17
    models:
      - provider: gemini
        model: gemini-2.5-flash-preview-04-17

  # type: "components.kapa.Kapa"
  # endpoint type can be "chat" or "search"
  # "search" uses https://docs.kapa.ai/api#tag/Search
  # docs say search is at: https://api.kapa.ai/query/v1/projects/:project_id/search/
  # "chat" uses https://docs.kapa.ai/api#tag/Chat/operation/query_v1_projects_chat
  # docs say chat is at: https://api.kapa.ai/query/v1/projects/:project_id/chat/

  # token: ${KAPA_API_TOKEN_EXTERNAL_DOCSBOT}

# vector_store:
#   type: qdrant
#   collection: make_this_parameterizable_per_api_call
#   host: 0.0.0.0
#   port: 6333
